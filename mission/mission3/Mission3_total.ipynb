{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024 데이터 크리에이터 캠프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제: 인공지능은 사람의 마음을 이해할수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission3. 패션스타일 선호 여부 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission 3-1.    \n",
    "추천 시스템의 기본인 협업 필터링 (Collaborative Filtering)은 크게 user-based filtering, item-based filtering 방식으로 나뉘어져 있다.  \n",
    "각각에 대해서 이해하고, 2-2에서 구해 본 응답자의 “스타일 선호 정보표”를 토대로 Validation 데이터 내 응답자의 “스타일 선호 여부 예측” 문제를  \n",
    "2가지 기법으로 어떻게 적용해 볼 수 있고, 서로 비교하여 어떤 장단점을 갖는지 설명한다.  \n",
    " ※ 설명을 용이하게 하기 위해 응답자의 스타일 선호도 예시를 들어서 설명해도 무방하다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "응답자의 \"스타일 선호 정보표\"를 토대로 validation 데이터 내 응답자의 스타일 선호 여부를 예측해볼 수 있습니다.   \n",
    "먼저 item based filtering을 적용한 방식에 대해 설명하겠습니다.    \n",
    "\n",
    "우선 평가가 이루어지지 않은 스타일에 대해서는 전부 제거해줍니다. 예를 들어 응답자 A는 비선호하는 스타일만 있을 뿐 선호하는 아이템이 없다면 선호하는    \n",
    "스타일이 있는지 확인해볼 수 없기 때문에 결측치는 전부 제거해준 뒤에 예측을 진행합니다.    \n",
    "\n",
    "다음으로 각 스타일의 특성을 고려한 벡터값을 resnet18 모델에 이미지를 넣어 구합니다. 이렇게 구한 특징 벡터는 스타일 간의 유사도를 계산하는데 사용됩니다.    \n",
    "각 응답자마다 선호, 비선호하는 스타일이 나뉘어져 있을 뿐만 아니라 선호하는 스타일의 수 또한 상이합니다. 어떤 응답자는 선호하는 스타일이 하나만 있거나    \n",
    "2개 이상의 아이템을 선호하는 사용자가 있기도 합니다. 또는 비선호하는 스타일이 여러개인 응답자도 존재한 반면 비선호하는 스타일이 하나만 있는 경우도 존재합니다.    \n",
    "이렇게 선호, 비선호하는 스타일의 수가 제각기 다르기 때문에 이를 하나의 특성 벡터로 만들어줍니다. 이 때 평균을 활용해서 응답자마다 선호, 비선호하는    \n",
    "스타일의 특징 벡터값을 도출합니다.     \n",
    "\n",
    "이제 마지막 단계로 validation 데이터의 특징 벡터값과 training 데이터의 특징 벡터값의 유사도 계산을 한 후에 선호하는 스타일과 유사도가 높으면     \n",
    "선호로 예측하고 비선호하는 스타일과 유사도가 높으면 비선호로 예측하는 방식으로 스타일 선호 여부를 예측합니다. 유사도 임계치는 사용자 임의로    \n",
    "설정할 수 있습니다.   \n",
    "\n",
    "마지막으로 예측한 값은 실제 정답과 비교를 통해 얼마나 정확하게 예측했는지는 confusion matrix에서 accuracty 값으로 확인할 수 있습니다.  \n",
    "user-based filtering을 적용한 방식에 대해 설명하겠습니다.    \n",
    "  \n",
    "우선, item-based filtering과 마찬가지로 해당 사용자가 평가하지 않은 아이템은 제거합니다.   \n",
    "다음으로, 각 사용자 간의 유사성을 분석하기 위해 사용자 간의 평가 데이터를 기반으로 유사도 매트릭스를 생성합니다.  \n",
    "이 과정에서는 코사인 유사도, 피어슨 상관계수 등의 방법을 사용하여 사용자의 선호 패턴을 비교합니다.  \n",
    "이를 통해 비슷한 취향을 가진 사용자 그룹을 식별할 수 있습니다.  \n",
    "\n",
    "\n",
    "각 사용자의 선호도는 다를 수 있으며, 이에 따라 선호하는 아이템의 수 또한 차이가 있습니다. 어떤 사용자는 선호하는 아이템이 많을 수도 있고,  \n",
    "어떤 사용자는 적을 수도 있습니다. 이러한 다양한 선호도를 반영하기 위해, 유사한 사용자들의 선호도를 평균하여 특정 사용자에 대한 예측을 생성합니다.  \n",
    "예를 들어, 유사한 사용자들이 높은 평점을 준 아이템은 해당 사용자에게도 추천될 가능성이 높습니다.  \n",
    "\n",
    "이제 마지막 단계로, validation 데이터에서 각 사용자의 선호도 벡터와 training 데이터에서 도출한 사용자 벡터 간의 유사도를 계산합니다.  \n",
    "유사도가 높은 사용자들로부터 추천 아이템을 선택하여, 해당 아이템이 선호될 가능성이 높은지 예측합니다. 이때 유사도 임계치는 사용자가 임의로 설정할 수 있습니다.  \n",
    "\n",
    "마지막으로, 예측한 값은 실제 정답과 비교하여 얼마나 정확하게 예측했는지를 confusion matrix를 통해 accuracy 값으로 확인할 수 있습니다.\n",
    "\n",
    "사용자 기반 협업 필터링과 아이템 기반 협업 필터링을 비교해보자면 우선 사용자 기반 협업 필터링의 경우 각 사용자의 선호도를 기반으로 추천을 제공하므로,  \n",
    "사용자 개개인에 맞춘 개인화된 결과를 생성할 수 있습니다. 또한 유사한 취향을 가진 여러 사용자들의 정보를 활용하므로 다양한 아이템을 추천할 수 있습니다.  \n",
    "그러나 사용자 수가 너무 많고 각 사용자가 평가한 아이템의 수가 너무 적을 경우 유사한 사용자 찾기가 어려워 추천의 질이 떨어질 수 있으며 사용자 수가 늘어날수록  \n",
    "계산 복잡도가 높아져 성능이 저하될 수 있습니다.  \n",
    "\n",
    "반면 아이템 기반 협업 필터링은 아이템 간의 유사성을 기반으로 하기 때문에, 사용자 수가 적더라도 추천이 가능합니다. 또한 사용자가 선호하는 아이템과 유사한  \n",
    "아이템을 추천하기 때문에 사용자는 쉽게 이해하고 수용할 수 있습니다. 단점으로는 아이템의 특성에만 집중하기 때문에 개별 사용자의 취향을 충분히 반영하지  \n",
    "못한다는 점이 있습니다. 또한 사용자 취향이 변화할 경우 아이템 간의 유사성 만으로는 추천을 하기 어려울 수 있습니다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mission 3-2.  \n",
    "3-1에서 살펴 본 기법 중, item-based filtering을 직접 구현해본다. “이미지 간 유사도” (image2image)만을 활용하여 Validation 데이터 내   \n",
    "응답자의 “스타일 선호 여부 예측” 문제를 수행하고 성능을 측정한다.    \n",
    " ※ Hint. 1-2에서 학습한 ResNet-18의 중간 layer 값을 활용하여 각 이미지의 feature vector를 구하고, 벡터 연산을 통해 이미지 간 유사도를 구해볼 수 있다.  \n",
    " ※ 예측 문제에서 활용한 파라미터 및 임계 값 등의 수치를 정확하게 제시한다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Type\n",
    "from collections import defaultdict\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        expansion: int = 1,\n",
    "        downsample: nn.Module = None\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels*self.expansion,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return  out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_channels: int,\n",
    "        num_layers: int,\n",
    "        block: Type[BasicBlock],\n",
    "        num_classes: int  = 1000\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if num_layers == 18: # ResNet18 만을 본 대회에서 사용함으로 18층만 구현\n",
    "            layers = [2, 2, 2, 2]\n",
    "            self.expansion = 1\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=img_channels,\n",
    "            out_channels=self.in_channels,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512*self.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[BasicBlock],\n",
    "        out_channels: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1\n",
    "    ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    out_channels*self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_channels, out_channels, stride, self.expansion, downsample\n",
    "            )\n",
    "        )\n",
    "        self.in_channels = out_channels * self.expansion\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(\n",
    "                self.in_channels,\n",
    "                out_channels,\n",
    "                expansion=self.expansion\n",
    "            ))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # print('Dimensions of the last convolutional feature map: ', x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-18 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-18 모델 정의\n",
    "class ResNet18FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18FeatureExtractor, self).__init__()\n",
    "        self.resnet18 = models.resnet18(pretrained=True) # 사전 학습된 가중치\n",
    "        self.features = nn.Sequential(*list(self.resnet18.children())[:-1])  # 마지막 FC 레이어 제외\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3480 entries, 0 to 3479\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   응답자 ID     3480 non-null   int64 \n",
      " 1   train 선호   2215 non-null   object\n",
      " 2   train 비선호  2830 non-null   object\n",
      " 3   valid 선호   1048 non-null   object\n",
      " 4   valid 비선호  1354 non-null   object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 136.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 불러오기\n",
    "mission2_result = pd.read_csv('../dataset/mission2-2_result_all.csv')\n",
    "\n",
    "# 데이터프레임의 일부 출력\n",
    "mission2_result.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>응답자 ID</th>\n",
       "      <th>train 선호</th>\n",
       "      <th>train 비선호</th>\n",
       "      <th>valid 선호</th>\n",
       "      <th>valid 비선호</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_00253_60_popart_W.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_00253_60_popart_W.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_00253_60_popart_W.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_00253_60_popart_W.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_00253_60_popart_W.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_00253_60_popart_W.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66592</td>\n",
       "      <td>T_00253_60_popart_W.jpg, T_00893_90_hiphop_W.j...</td>\n",
       "      <td>T_07452_50_classic_W.jpg, W_02170_50_feminine_...</td>\n",
       "      <td>T_00253_60_popart_W.jpg, W_10028_50_classic_W....</td>\n",
       "      <td>W_02170_50_feminine_W.jpg, W_19352_50_feminine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66721</td>\n",
       "      <td>W_05960_70_hippie_W.jpg</td>\n",
       "      <td>T_00253_60_popart_W.jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T_00253_60_popart_W.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   응답자 ID                                           train 선호  \\\n",
       "0   58049                                                NaN   \n",
       "1   62192                                                NaN   \n",
       "2   64213                                                NaN   \n",
       "3   66592  T_00253_60_popart_W.jpg, T_00893_90_hiphop_W.j...   \n",
       "4   66721                            W_05960_70_hippie_W.jpg   \n",
       "\n",
       "                                           train 비선호  \\\n",
       "0                            T_00253_60_popart_W.jpg   \n",
       "1                            T_00253_60_popart_W.jpg   \n",
       "2                            T_00253_60_popart_W.jpg   \n",
       "3  T_07452_50_classic_W.jpg, W_02170_50_feminine_...   \n",
       "4                            T_00253_60_popart_W.jpg   \n",
       "\n",
       "                                            valid 선호  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3  T_00253_60_popart_W.jpg, W_10028_50_classic_W....   \n",
       "4                                                NaN   \n",
       "\n",
       "                                           valid 비선호  \n",
       "0                            T_00253_60_popart_W.jpg  \n",
       "1                            T_00253_60_popart_W.jpg  \n",
       "2                            T_00253_60_popart_W.jpg  \n",
       "3  W_02170_50_feminine_W.jpg, W_19352_50_feminine...  \n",
       "4                            T_00253_60_popart_W.jpg  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mission2_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train 데이터에서 선호/비선호 여부에 따라 데이터 있는경우에 추천이 가능하다고 판단하였고  \n",
    "- valid 데이터의 또한 선호/비선호 데이터가 모두 있는 경우를 사용  \n",
    "-> valid 데이터의 경우 선호/비선호 값에 속하는 이미지가 모두 존재해야되지는 않지만 통일성을 위해  \n",
    "데이터가 모두 존재하는 상황에서 아이템 기반 필터링(Item-Based Filtering) 방식 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>응답자 ID</th>\n",
       "      <th>train 선호</th>\n",
       "      <th>train 비선호</th>\n",
       "      <th>valid 선호</th>\n",
       "      <th>valid 비선호</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66592</td>\n",
       "      <td>T_00253_60_popart_W.jpg, T_00893_90_hiphop_W.j...</td>\n",
       "      <td>T_07452_50_classic_W.jpg, W_02170_50_feminine_...</td>\n",
       "      <td>T_00253_60_popart_W.jpg, W_10028_50_classic_W....</td>\n",
       "      <td>W_02170_50_feminine_W.jpg, W_19352_50_feminine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66469</td>\n",
       "      <td>T_00456_10_sportivecasual_M.jpg, T_00588_10_sp...</td>\n",
       "      <td>T_02958_19_normcore_M.jpg, T_06076_60_mods_M.j...</td>\n",
       "      <td>T_00456_10_sportivecasual_M.jpg, T_01123_90_hi...</td>\n",
       "      <td>W_24553_70_hippie_M.jpg, W_24647_70_hippie_M.j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66513</td>\n",
       "      <td>T_05088_19_normcore_W.jpg, T_07416_19_lounge_W...</td>\n",
       "      <td>T_08306_10_sportivecasual_W.jpg, T_08918_19_no...</td>\n",
       "      <td>W_14828_50_classic_W.jpg</td>\n",
       "      <td>T_06910_50_classic_W.jpg, W_10984_50_feminine_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58251</td>\n",
       "      <td>T_08242_10_sportivecasual_W.jpg, T_08663_19_no...</td>\n",
       "      <td>T_11058_90_lingerie_W.jpg, T_12089_80_powersui...</td>\n",
       "      <td>T_14538_00_cityglam_W.jpg, W_00716_60_minimal_...</td>\n",
       "      <td>W_04101_19_lounge_W.jpg, W_08886_10_athleisure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48094</td>\n",
       "      <td>T_14538_00_cityglam_W.jpg</td>\n",
       "      <td>W_09479_70_hippie_W.jpg</td>\n",
       "      <td>T_14538_00_cityglam_W.jpg</td>\n",
       "      <td>W_09479_70_hippie_W.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>64621</td>\n",
       "      <td>W_29596_10_sportivecasual_M.jpg</td>\n",
       "      <td>W_24250_90_hiphop_M.jpg</td>\n",
       "      <td>W_28925_90_hiphop_M.jpg</td>\n",
       "      <td>W_24250_90_hiphop_M.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>66842</td>\n",
       "      <td>W_27765_60_mods_M.jpg</td>\n",
       "      <td>W_24352_70_hippie_M.jpg</td>\n",
       "      <td>W_27765_60_mods_M.jpg</td>\n",
       "      <td>W_16485_00_metrosexual_M.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>64772</td>\n",
       "      <td>W_32595_60_mods_M.jpg, W_48687_60_mods_M.jpg</td>\n",
       "      <td>W_24470_70_hippie_M.jpg, W_24590_60_mods_M.jpg...</td>\n",
       "      <td>W_33329_50_ivy_M.jpg, W_48687_60_mods_M.jpg</td>\n",
       "      <td>W_24590_60_mods_M.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>64828</td>\n",
       "      <td>W_35091_80_powersuit_W.jpg</td>\n",
       "      <td>W_34027_10_sportivecasual_W.jpg</td>\n",
       "      <td>W_22783_70_hippie_W.jpg, W_35091_80_powersuit_...</td>\n",
       "      <td>W_34027_10_sportivecasual_W.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>67669</td>\n",
       "      <td>W_71920_60_mods_M.jpg</td>\n",
       "      <td>W_52548_50_ivy_M.jpg, W_52586_50_ivy_M.jpg</td>\n",
       "      <td>W_52578_50_ivy_M.jpg</td>\n",
       "      <td>W_52521_50_ivy_M.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     응답자 ID                                           train 선호  \\\n",
       "0     66592  T_00253_60_popart_W.jpg, T_00893_90_hiphop_W.j...   \n",
       "1     66469  T_00456_10_sportivecasual_M.jpg, T_00588_10_sp...   \n",
       "2     66513  T_05088_19_normcore_W.jpg, T_07416_19_lounge_W...   \n",
       "3     58251  T_08242_10_sportivecasual_W.jpg, T_08663_19_no...   \n",
       "4     48094                          T_14538_00_cityglam_W.jpg   \n",
       "..      ...                                                ...   \n",
       "463   64621                    W_29596_10_sportivecasual_M.jpg   \n",
       "464   66842                              W_27765_60_mods_M.jpg   \n",
       "465   64772       W_32595_60_mods_M.jpg, W_48687_60_mods_M.jpg   \n",
       "466   64828                         W_35091_80_powersuit_W.jpg   \n",
       "467   67669                              W_71920_60_mods_M.jpg   \n",
       "\n",
       "                                             train 비선호  \\\n",
       "0    T_07452_50_classic_W.jpg, W_02170_50_feminine_...   \n",
       "1    T_02958_19_normcore_M.jpg, T_06076_60_mods_M.j...   \n",
       "2    T_08306_10_sportivecasual_W.jpg, T_08918_19_no...   \n",
       "3    T_11058_90_lingerie_W.jpg, T_12089_80_powersui...   \n",
       "4                              W_09479_70_hippie_W.jpg   \n",
       "..                                                 ...   \n",
       "463                            W_24250_90_hiphop_M.jpg   \n",
       "464                            W_24352_70_hippie_M.jpg   \n",
       "465  W_24470_70_hippie_M.jpg, W_24590_60_mods_M.jpg...   \n",
       "466                    W_34027_10_sportivecasual_W.jpg   \n",
       "467         W_52548_50_ivy_M.jpg, W_52586_50_ivy_M.jpg   \n",
       "\n",
       "                                              valid 선호  \\\n",
       "0    T_00253_60_popart_W.jpg, W_10028_50_classic_W....   \n",
       "1    T_00456_10_sportivecasual_M.jpg, T_01123_90_hi...   \n",
       "2                             W_14828_50_classic_W.jpg   \n",
       "3    T_14538_00_cityglam_W.jpg, W_00716_60_minimal_...   \n",
       "4                            T_14538_00_cityglam_W.jpg   \n",
       "..                                                 ...   \n",
       "463                            W_28925_90_hiphop_M.jpg   \n",
       "464                              W_27765_60_mods_M.jpg   \n",
       "465        W_33329_50_ivy_M.jpg, W_48687_60_mods_M.jpg   \n",
       "466  W_22783_70_hippie_W.jpg, W_35091_80_powersuit_...   \n",
       "467                               W_52578_50_ivy_M.jpg   \n",
       "\n",
       "                                             valid 비선호  \n",
       "0    W_02170_50_feminine_W.jpg, W_19352_50_feminine...  \n",
       "1    W_24553_70_hippie_M.jpg, W_24647_70_hippie_M.j...  \n",
       "2    T_06910_50_classic_W.jpg, W_10984_50_feminine_...  \n",
       "3    W_04101_19_lounge_W.jpg, W_08886_10_athleisure...  \n",
       "4                              W_09479_70_hippie_W.jpg  \n",
       "..                                                 ...  \n",
       "463                            W_24250_90_hiphop_M.jpg  \n",
       "464                       W_16485_00_metrosexual_M.jpg  \n",
       "465                              W_24590_60_mods_M.jpg  \n",
       "466                    W_34027_10_sportivecasual_W.jpg  \n",
       "467                               W_52521_50_ivy_M.jpg  \n",
       "\n",
       "[468 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 열의 값이 채워져 있는 행만 선택\n",
    "filtered_mission2_result = mission2_result.dropna()\n",
    "\n",
    "# 인덱스 재설정\n",
    "filtered_mission2_result = filtered_mission2_result.reset_index(drop=True)\n",
    "\n",
    "# 결과 출력\n",
    "filtered_mission2_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이미지 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-18 모델을 사용하여 이미지의 feature vector를 추출 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=31, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 이미지 전처리\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 이미지 feature vector 추출 함수\n",
    "def extract_features(image_path, model, transform):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"File not found: {image_path}\")\n",
    "        return None\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    return features.numpy().flatten()\n",
    "\n",
    "# ResNet-18 모델 초기화\n",
    "class ResNet18FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18FeatureExtractor, self).__init__()\n",
    "        self.resnet18 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "        self.resnet18.fc = torch.nn.Identity()  # 마지막 FC 레이어를 Identity로 대체\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet18(x)\n",
    "\n",
    "# 저장된 모델 전체 로드\n",
    "model_path = r'../model/model_path.pth'\n",
    "model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지 간 유사도를 계산 및 Validation 데이터 내 응답자의 스타일 선호 여부를 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 학습 이미지와 검증 이미지의 디렉토리 경로 설정\n",
    "train_image_directory = '../dataset/training_image'\n",
    "valid_image_directory = '../dataset/validation_image'\n",
    "\n",
    "# 유사도 계산 함수\n",
    "def calculate_similarity(feature1, feature2):\n",
    "    return cosine_similarity([feature1], [feature2])[0][0]\n",
    "\n",
    "# 응답자별 학습 이미지와 검증 이미지의 feature vector 추출 및 저장\n",
    "def extract_respondent_features(mission2_result, model, transform):\n",
    "    respondent_train_features = {}\n",
    "    respondent_valid_features = {}\n",
    "\n",
    "    for index, row in mission2_result.iterrows():\n",
    "        respondent_id = row['응답자 ID']\n",
    "        train_images = []\n",
    "        valid_images = []\n",
    "\n",
    "        if not pd.isna(row['train 선호']):\n",
    "            train_images.extend([(img, '선호') for img in row['train 선호'].split(', ')])\n",
    "        if not pd.isna(row['train 비선호']):\n",
    "            train_images.extend([(img, '비선호') for img in row['train 비선호'].split(', ')])\n",
    "        if not pd.isna(row['valid 선호']):\n",
    "            valid_images.extend(row['valid 선호'].split(', '))\n",
    "        if not pd.isna(row['valid 비선호']):\n",
    "            valid_images.extend(row['valid 비선호'].split(', '))\n",
    "\n",
    "        respondent_train_features[respondent_id] = {}\n",
    "        respondent_valid_features[respondent_id] = {}\n",
    "\n",
    "        for train_image, preference in train_images:\n",
    "            image_path = os.path.join(train_image_directory, train_image).replace('\\\\', '/')\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"File not found: {image_path}\")\n",
    "                continue\n",
    "            image_id = train_image.split('_')[1]\n",
    "            feature = extract_features(image_path, model, transform)\n",
    "            if feature is not None:\n",
    "                respondent_train_features[respondent_id][image_id] = (feature, preference)\n",
    "\n",
    "        for valid_image in valid_images:\n",
    "            image_path = os.path.join(valid_image_directory, valid_image).replace('\\\\', '/')\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"File not found: {image_path}\")\n",
    "                continue\n",
    "            image_id = valid_image.split('_')[1]\n",
    "            feature = extract_features(image_path, model, transform)\n",
    "            if feature is not None:\n",
    "                respondent_valid_features[respondent_id][image_id] = feature\n",
    "\n",
    "    return respondent_train_features, respondent_valid_features\n",
    "\n",
    "# 응답자별 학습 이미지와 검증 이미지의 feature vector 추출\n",
    "respondent_train_features, respondent_valid_features = extract_respondent_features(filtered_mission2_result, model, transform)\n",
    "\n",
    "# Validation 데이터 내 응답자의 스타일 선호 여부 예측\n",
    "def predict_preference(respondent_train_features, respondent_valid_features, threshold=0.5):\n",
    "    predictions = {}\n",
    "    for respondent_id, valid_features in respondent_valid_features.items():\n",
    "        respondent_predictions = {}\n",
    "        for valid_image_id, valid_feature in valid_features.items():\n",
    "            similarities = {'선호': [], '비선호': []}\n",
    "            for train_image_id, (train_feature, preference) in respondent_train_features[respondent_id].items():\n",
    "                similarity = calculate_similarity(valid_feature, train_feature)\n",
    "                similarities[preference].append(similarity)\n",
    "            # 모든 유사도를 사용하여 평균 계산\n",
    "            if len(similarities['선호']) == 0 and len(similarities['비선호']) == 0:\n",
    "                preference_score = 0  # 기본값 설정\n",
    "            else:\n",
    "                preference_score = (sum(similarities['선호']) - sum(similarities['비선호'])) / (len(similarities['선호']) + len(similarities['비선호']))\n",
    "            respondent_predictions[valid_image_id] = '선호' if preference_score > threshold else '비선호'\n",
    "        predictions[respondent_id] = respondent_predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_preference(respondent_train_features, respondent_valid_features, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.55\n"
     ]
    }
   ],
   "source": [
    "# 성능 측정 (예시로 정확도 계산)\n",
    "def calculate_accuracy(predictions, mission2_result):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for index, row in mission2_result.iterrows():\n",
    "        respondent_id = row['응답자 ID']\n",
    "        if respondent_id not in predictions:\n",
    "            continue\n",
    "        for valid_image in row['valid 선호'].split(', ') if not pd.isna(row['valid 선호']) else []:\n",
    "            valid_image_id = valid_image.split('_')[1]\n",
    "            if valid_image_id in predictions[respondent_id]:\n",
    "                total += 1\n",
    "                if predictions[respondent_id][valid_image_id] == '선호':\n",
    "                    correct += 1\n",
    "        for valid_image in row['valid 비선호'].split(', ') if not pd.isna(row['valid 비선호']) else []:\n",
    "            valid_image_id = valid_image.split('_')[1]\n",
    "            if valid_image_id in predictions[respondent_id]:\n",
    "                total += 1\n",
    "                if predictions[respondent_id][valid_image_id] == '비선호':\n",
    "                    correct += 1\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = calculate_accuracy(predictions, mission2_result)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
