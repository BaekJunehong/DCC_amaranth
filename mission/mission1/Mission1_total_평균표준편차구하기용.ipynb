{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGiVd07r_Jdw"
   },
   "source": [
    "# 2024 데이터 크리에이터 캠프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nFcUiVl_Jd0"
   },
   "source": [
    "문제: 인공지능은 사람의 마음을 이해할수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4X7RQ3lkWq5L"
   },
   "source": [
    "### 구글 드라이브 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22675,
     "status": "ok",
     "timestamp": 1730257944874,
     "user": {
      "displayName": "‍백준홍[재학 / 통계학과]",
      "userId": "06122431454026888440"
     },
     "user_tz": -540
    },
    "id": "a5yiwx3lWk3t",
    "outputId": "37b66612-a84c-48e1-d078-04432afce22a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEZZIiCO_Jd1"
   },
   "source": [
    "## Mission1. 패션 스타일 이미지 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6omIic-_Jd7"
   },
   "source": [
    "### Mission 1-2.  \n",
    "ResNet-18를 활용하여 “성별 & 스타일” 단위로 클래스 분류를 수행하고 Validation 데이터에 대한 정확도를 제시한다.   \n",
    " - ResNet-18의 parameters는 무작위로 초기화하여 사용한다. (즉, pretrained weights는 사용할 수 없음)  \n",
    " - 성능을 높이기 위해 object detection, image cropping 등의 다양한 데이터 전처리 기법을 활용해도 무방하다.    \n",
    " (데이터 전처리 단계에 한해서는 외부 라이브러리 활용 가능)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggtOkjTD_Jd8"
   },
   "source": [
    "### 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9orNrymiUL4k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Byt9U7oG_Jd8"
   },
   "source": [
    "### Resnet 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47HNcz9LsmMl"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        stride: int = 1,\n",
    "        expansion: int = 1,\n",
    "        downsample: nn.Module = None\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels,\n",
    "            out_channels*self.expansion,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels*self.expansion)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return  out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kFiXzxVsmKJ"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_channels: int,\n",
    "        num_layers: int,\n",
    "        block: Type[BasicBlock],\n",
    "        num_classes: int  = 1000\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if num_layers == 18: # ResNet18 만을 본 대회에서 사용함으로 18층만 구현\n",
    "            layers = [2, 2, 2, 2]\n",
    "            self.expansion = 1\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=img_channels,\n",
    "            out_channels=self.in_channels,\n",
    "            kernel_size=7,\n",
    "            stride=2,\n",
    "            padding=3,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512*self.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[BasicBlock],\n",
    "        out_channels: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1\n",
    "    ) -> nn.Sequential:\n",
    "        downsample = None\n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    out_channels*self.expansion,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.in_channels, out_channels, stride, self.expansion, downsample\n",
    "            )\n",
    "        )\n",
    "        self.in_channels = out_channels * self.expansion\n",
    "\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(\n",
    "                self.in_channels,\n",
    "                out_channels,\n",
    "                expansion=self.expansion\n",
    "            ))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # print('Dimensions of the last convolutional feature map: ', x.shape)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yC3L0D6u_Jd9"
   },
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WsmgBLi_Jd9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything()\n",
    "\n",
    "# 이미지 파일이 있는 디렉토리 경로\n",
    "train_image_directory = '/content/drive/MyDrive/kict/dataset/training_image'   # <--- training_image 폴더 경로를 입력\n",
    "valid_image_directory = '/content/drive/MyDrive/kict/dataset/validation_image' # <--- validation_image 폴더 경로를 입력\n",
    "\n",
    "# CustomDataset 클래스 정의\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_directory, transform=None):\n",
    "        self.image_directory = image_directory\n",
    "        self.image_files = [f for f in os.listdir(image_directory) if f.endswith('.jpg')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.image_directory, file_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        # 이미지 파일명에서 스타일과 성별 정보 추출\n",
    "        parts = file_name.split('_')\n",
    "        style_gender = parts[-2] + '_' + parts[-1].split('.')[0]  # 스타일과 성별 정보 추출\n",
    "\n",
    "        # 스타일과 성별 정보를 레이블로 변환\n",
    "        label = style_gender\n",
    "        label_idx = label_to_index[label]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label_idx\n",
    "\n",
    "# 레이블을 숫자로 매핑하기 위한 딕셔너리 생성\n",
    "label_set = set()\n",
    "for file_name in os.listdir(train_image_directory) + os.listdir(valid_image_directory):\n",
    "    parts = file_name.split('_')\n",
    "    style_gender = parts[-2] + '_' + parts[-1].split('.')[0]\n",
    "    label_set.add(style_gender)\n",
    "\n",
    "label_list = sorted(list(label_set))\n",
    "label_to_index = {label: idx for idx, label in enumerate(label_list)}\n",
    "index_to_label = {idx: label for label, idx in label_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1730257979182,
     "user": {
      "displayName": "‍백준홍[재학 / 통계학과]",
      "userId": "06122431454026888440"
     },
     "user_tz": -540
    },
    "id": "BS5EFB7h_Jd9",
    "outputId": "3f6dcbaa-8dcc-45f0-ede2-fb457b32498a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'athleisure_W',\n",
       " 1: 'bodyconscious_W',\n",
       " 2: 'bold_M',\n",
       " 3: 'cityglam_W',\n",
       " 4: 'classic_W',\n",
       " 5: 'disco_W',\n",
       " 6: 'ecology_W',\n",
       " 7: 'feminine_W',\n",
       " 8: 'genderless_W',\n",
       " 9: 'grunge_W',\n",
       " 10: 'hiphop_M',\n",
       " 11: 'hiphop_W',\n",
       " 12: 'hippie_M',\n",
       " 13: 'hippie_W',\n",
       " 14: 'ivy_M',\n",
       " 15: 'kitsch_W',\n",
       " 16: 'lingerie_W',\n",
       " 17: 'lounge_W',\n",
       " 18: 'metrosexual_M',\n",
       " 19: 'military_W',\n",
       " 20: 'minimal_W',\n",
       " 21: 'mods_M',\n",
       " 22: 'normcore_M',\n",
       " 23: 'normcore_W',\n",
       " 24: 'oriental_W',\n",
       " 25: 'popart_W',\n",
       " 26: 'powersuit_W',\n",
       " 27: 'punk_W',\n",
       " 28: 'space_W',\n",
       " 29: 'sportivecasual_M',\n",
       " 30: 'sportivecasual_W'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTJ6EW0Z_Jd-"
   },
   "source": [
    "클래스 총 31개의 범주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730257979182,
     "user": {
      "displayName": "‍백준홍[재학 / 통계학과]",
      "userId": "06122431454026888440"
     },
     "user_tz": -540
    },
    "id": "ZYRCPlPg_Jd-",
    "outputId": "41a93e39-47fe-4922-a00e-c427d6fdac99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHWZG2R1_Jd-"
   },
   "source": [
    "이미지 데이터 정규화를 위한 평균과 표준편차 계산 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3425600,
     "status": "ok",
     "timestamp": 1730261404778,
     "user": {
      "displayName": "‍백준홍[재학 / 통계학과]",
      "userId": "06122431454026888440"
     },
     "user_tz": -540
    },
    "id": "ytsLW5MT_Jd-",
    "outputId": "e46bbda9-e3cc-46fa-9f18-4bc1f55d1b4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean: tensor([0.5498, 0.5226, 0.5052])\n",
      "Calculated std: tensor([0.2600, 0.2582, 0.2620])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 평균과 표준편차 계산 함수\n",
    "def calculate_mean_std(loader):\n",
    "    mean = 0.0\n",
    "    std = 0.0\n",
    "    total_images_count = 0\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)  # 배치 크기 (이때 마지막 배치는 더 작을 수 있음)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += batch_samples\n",
    "\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    return mean, std\n",
    "\n",
    "# 임시로 ToTensor 변환만 적용하여 데이터 로더 생성\n",
    "temp_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "temp_train_dataset = CustomDataset(train_image_directory, transform=temp_transform)\n",
    "temp_train_loader = DataLoader(temp_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 평균과 표준편차 계산\n",
    "mean, std = calculate_mean_std(temp_train_loader)\n",
    "print(f\"Calculated mean: {mean}\")\n",
    "print(f\"Calculated std: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO9K44Y0_Jd_"
   },
   "source": [
    "데이터 전처리 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_L1sMbdx_Jd_"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2,\n",
    "                           contrast=0.2,\n",
    "                           saturation=0.2,\n",
    "                           hue=0.1),\n",
    "    transforms.ToTensor(), # 이미지의 픽셀 값이 [0, 255] 범위에서 [0, 1] 범위로 정규화\n",
    "    transforms.Normalize(  # 텐서의 각 채널을 주어진 평균과 표준편차를 사용하여 정규화\n",
    "        mean=mean.tolist(),\n",
    "        std=std.tolist()\n",
    "        )\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_image_directory, transform=transform)\n",
    "val_dataset = CustomDataset(valid_image_directory, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrvwttFr_JeF"
   },
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
